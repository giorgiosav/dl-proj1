{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlc_practical_prologue as plg\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from operator import mul as multiplicator\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = plg.generate_pair_sets(N)\n",
    "train_input = train_input.to(device)\n",
    "train_target = train_target.to(device)\n",
    "train_classes = train_classes.to(device)\n",
    "test_input = test_input.to(device)\n",
    "test_target = test_target.to(device)\n",
    "test_classes = test_classes.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "mu, std = train_input.mean(), train_input.std()\n",
    "train_input = train_input.sub(mu).div(std)\n",
    "test_input = test_input.sub(mu).div(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: simple network trained with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, nb_hidden1 = 50, nb_hidden2 = 10):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=3, padding=2)\n",
    "        self.avgpool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, dilation=1)\n",
    "        #self.conv3 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(16 * 3 * 3, nb_hidden1)\n",
    "        self.fc2 = nn.Linear(nb_hidden1, nb_hidden2)\n",
    "        self.fc3 = nn.Linear(nb_hidden2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.avgpool1(self.conv1(x)))\n",
    "        x = F.relu(self.maxpool1(self.conv2(x)))\n",
    "        x = F.relu(self.fc1(x.view(-1, 16*3*3)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self, nb_hidden1 = 50, nb_hidden2 = 10):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=3, groups=2)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, dilation=1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=2)\n",
    "        self.fc1 = nn.Linear(32*2*2, nb_hidden1)\n",
    "        self.fc2 = nn.Linear(nb_hidden1, nb_hidden2)\n",
    "        self.fc3 = nn.Linear(nb_hidden2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.maxpool1(self.conv1(x)))\n",
    "        x = F.relu(self.maxpool2(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x.view(-1, 32*2*2)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2)) output shape:\t torch.Size([1, 8, 16, 16])\n",
      "AvgPool2d(kernel_size=2, stride=2, padding=0) output shape:\t torch.Size([1, 8, 8, 8])\n",
      "Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1)) output shape:\t torch.Size([1, 16, 6, 6])\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) output shape:\t torch.Size([1, 16, 3, 3])\n",
      "Linear(in_features=144, out_features=50, bias=True) output shape:\t torch.Size([1, 50])\n",
      "Linear(in_features=50, out_features=10, bias=True) output shape:\t torch.Size([1, 10])\n",
      "Linear(in_features=10, out_features=2, bias=True) output shape:\t torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# Check on the sizes\n",
    "X = torch.empty((1, 2, 14, 14)).normal_()\n",
    "net = Net()\n",
    "for layer in net.children():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        X = X.view(-1, reduce(multiplicator, list(X.shape[1:])))\n",
    "    X = layer(X)\n",
    "    print(layer, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), groups=2) output shape:\t torch.Size([1, 8, 12, 12])\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) output shape:\t torch.Size([1, 8, 6, 6])\n",
      "Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) output shape:\t torch.Size([1, 16, 6, 6])\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) output shape:\t torch.Size([1, 16, 3, 3])\n",
      "Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1)) output shape:\t torch.Size([1, 32, 2, 2])\n",
      "Linear(in_features=128, out_features=50, bias=True) output shape:\t torch.Size([1, 50])\n",
      "Linear(in_features=50, out_features=10, bias=True) output shape:\t torch.Size([1, 10])\n",
      "Linear(in_features=10, out_features=2, bias=True) output shape:\t torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# Check on the sizes\n",
    "X = torch.empty((1, 2, 14, 14)).normal_()\n",
    "net = Net2()\n",
    "for layer in net.children():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        X = X.view(-1, reduce(multiplicator, list(X.shape[1:])))\n",
    "    X = layer(X)\n",
    "    print(layer, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size, criterion, epochs, eta, optim=\"SGD\", momentum = 0, nesterov = False):\n",
    "    \n",
    "    if (optim == \"SGD\"):\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr = eta, momentum = momentum, nesterov = nesterov)\n",
    "    if (optim == \"Adam\"):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = eta)\n",
    "        \n",
    "    for e in range(0, epochs):\n",
    "        for input_data, target_data in zip(train_input.split(mini_batch_size), train_target.split(mini_batch_size)):\n",
    "            output = model(input_data)\n",
    "            loss = criterion(output, target_data)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):\n",
    "    tot_err = 0\n",
    "    for input_data, target_data in zip(data_input.split(mini_batch_size), data_target.split(mini_batch_size)):\n",
    "        res = model(input_data)\n",
    "        for i, r in enumerate(res):\n",
    "            pred = r.max(0)[1].item()\n",
    "            if(target_data[i])!=pred:\n",
    "                tot_err+=1\n",
    "    return tot_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "mini_batch_size = 100\n",
    "epochs = 250\n",
    "eta = 0.01\n",
    "train_model(model, train_input, train_target, mini_batch_size, criterion, epochs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors(model, train_input, train_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors(model, test_input, test_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize over eta (simple optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta = 1e-05, avg_err = 500.0\n",
      "Eta = 0.0001, avg_err = 489.6\n",
      "Eta = 0.001, avg_err = 494.1\n",
      "Eta = 0.01, avg_err = 462.8\n",
      "Eta = 0.1, avg_err = 215.1\n"
     ]
    }
   ],
   "source": [
    "mini_batch_size = 100\n",
    "epochs = 100\n",
    "etas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "N = 10\n",
    "for eta in etas:\n",
    "    tot_eta = 0\n",
    "    for i in range(0, N):\n",
    "        model = Net().to(device)\n",
    "        criterion = nn.CrossEntropyLoss().to(device)\n",
    "        train_model(model, train_input, train_target, mini_batch_size, criterion, epochs, eta)\n",
    "        err = compute_nb_errors(model, test_input, test_target, mini_batch_size)\n",
    "        tot_eta+=err\n",
    "    print(\"Eta = {}, avg_err = {}\".format(eta, tot_eta/N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize over eta, momentum, nesterov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta = 0.0001, momentum = 0.0001, nesterov = False, avg_err = 501.3333333333333\n",
      "Eta = 0.0001, momentum = 0.0001, nesterov = True, avg_err = 491.3333333333333\n",
      "Eta = 0.0001, momentum = 0.001, nesterov = False, avg_err = 482.0\n",
      "Eta = 0.0001, momentum = 0.001, nesterov = True, avg_err = 491.3333333333333\n",
      "Eta = 0.0001, momentum = 0.01, nesterov = False, avg_err = 491.3333333333333\n",
      "Eta = 0.0001, momentum = 0.01, nesterov = True, avg_err = 474.0\n",
      "Eta = 0.0001, momentum = 0.1, nesterov = False, avg_err = 491.3333333333333\n",
      "Eta = 0.0001, momentum = 0.1, nesterov = True, avg_err = 487.0\n",
      "Eta = 0.001, momentum = 0.0001, nesterov = False, avg_err = 508.3333333333333\n",
      "Eta = 0.001, momentum = 0.0001, nesterov = True, avg_err = 508.6666666666667\n",
      "Eta = 0.001, momentum = 0.001, nesterov = False, avg_err = 473.6666666666667\n",
      "Eta = 0.001, momentum = 0.001, nesterov = True, avg_err = 474.0\n",
      "Eta = 0.001, momentum = 0.01, nesterov = False, avg_err = 491.3333333333333\n",
      "Eta = 0.001, momentum = 0.01, nesterov = True, avg_err = 474.0\n",
      "Eta = 0.001, momentum = 0.1, nesterov = False, avg_err = 491.3333333333333\n",
      "Eta = 0.001, momentum = 0.1, nesterov = True, avg_err = 476.0\n",
      "Eta = 0.01, momentum = 0.0001, nesterov = False, avg_err = 470.6666666666667\n",
      "Eta = 0.01, momentum = 0.0001, nesterov = True, avg_err = 451.6666666666667\n",
      "Eta = 0.01, momentum = 0.001, nesterov = False, avg_err = 412.0\n",
      "Eta = 0.01, momentum = 0.001, nesterov = True, avg_err = 472.6666666666667\n",
      "Eta = 0.01, momentum = 0.01, nesterov = False, avg_err = 474.0\n",
      "Eta = 0.01, momentum = 0.01, nesterov = True, avg_err = 474.0\n",
      "Eta = 0.01, momentum = 0.1, nesterov = False, avg_err = 455.0\n",
      "Eta = 0.01, momentum = 0.1, nesterov = True, avg_err = 433.0\n",
      "Eta = 0.1, momentum = 0.0001, nesterov = False, avg_err = 207.0\n",
      "Eta = 0.1, momentum = 0.0001, nesterov = True, avg_err = 217.0\n",
      "Eta = 0.1, momentum = 0.001, nesterov = False, avg_err = 225.0\n",
      "Eta = 0.1, momentum = 0.001, nesterov = True, avg_err = 213.33333333333334\n",
      "Eta = 0.1, momentum = 0.01, nesterov = False, avg_err = 209.33333333333334\n",
      "Eta = 0.1, momentum = 0.01, nesterov = True, avg_err = 220.0\n",
      "Eta = 0.1, momentum = 0.1, nesterov = False, avg_err = 197.66666666666666\n",
      "Eta = 0.1, momentum = 0.1, nesterov = True, avg_err = 213.0\n"
     ]
    }
   ],
   "source": [
    "mini_batch_size = 100\n",
    "epochs = 100\n",
    "etas = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "momentum = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "N = 3\n",
    "for eta in etas:\n",
    "    for m in momentum:\n",
    "        for nest in [False, True]:\n",
    "            tot_eta = 0\n",
    "            for i in range(0, N):\n",
    "                model = Net().to(device)\n",
    "                criterion = nn.CrossEntropyLoss().to(device)\n",
    "                train_model(model, train_input, train_target, mini_batch_size, criterion, epochs, eta, \"SGD\", m, nest)\n",
    "                err = compute_nb_errors(model, test_input, test_target, mini_batch_size)\n",
    "                tot_eta+=err\n",
    "            print(\"Eta = {}, momentum = {}, nesterov = {}, avg_err = {}\".format(eta, m, nest, tot_eta/N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "mini_batch_size = 100\n",
    "epochs = 250\n",
    "eta = 0.01\n",
    "train_model(model, train_input, train_target, mini_batch_size, criterion, epochs, eta, \"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors(model, train_input, train_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors(model, test_input, test_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize over eta (simple optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta = 1e-05, avg_err = 480.7\n",
      "Eta = 0.0001, avg_err = 210.5\n",
      "Eta = 0.001, avg_err = 194.0\n",
      "Eta = 0.01, avg_err = 211.0\n",
      "Eta = 0.1, avg_err = 474.0\n"
     ]
    }
   ],
   "source": [
    "mini_batch_size = 100\n",
    "epochs = 100\n",
    "etas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "N = 10\n",
    "for eta in etas:\n",
    "    tot_eta = 0\n",
    "    for i in range(0, N):\n",
    "        model = Net().to(device)\n",
    "        criterion = nn.CrossEntropyLoss().to(device)\n",
    "        train_model(model, train_input, train_target, mini_batch_size, criterion, epochs, eta, \"Adam\")\n",
    "        err = compute_nb_errors(model, test_input, test_target, mini_batch_size)\n",
    "        tot_eta+=err\n",
    "    print(\"Eta = {}, avg_err = {}\".format(eta, tot_eta/N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net2().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "mini_batch_size = 100\n",
    "epochs = 250\n",
    "eta = 0.01\n",
    "train_model(model, train_input, train_target, mini_batch_size, criterion, epochs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors(model, train_input, train_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors(model, test_input, test_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize over eta (simple optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta = 1e-05, avg_err = 489.6\n",
      "Eta = 0.0001, avg_err = 498.4\n",
      "Eta = 0.001, avg_err = 489.5\n",
      "Eta = 0.01, avg_err = 456.1\n",
      "Eta = 0.1, avg_err = 213.6\n"
     ]
    }
   ],
   "source": [
    "mini_batch_size = 100\n",
    "epochs = 100\n",
    "etas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "N = 10\n",
    "for eta in etas:\n",
    "    tot_eta = 0\n",
    "    for i in range(0, N):\n",
    "        model = Net2().to(device)\n",
    "        criterion = nn.CrossEntropyLoss().to(device)\n",
    "        train_model(model, train_input, train_target, mini_batch_size, criterion, epochs, eta)\n",
    "        err = compute_nb_errors(model, test_input, test_target, mini_batch_size)\n",
    "        tot_eta+=err\n",
    "    print(\"Eta = {}, avg_err = {}\".format(eta, tot_eta/N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize over eta, momentum, nesterov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta = 0.0001, momentum = 0.0001, nesterov = False, avg_err = 508.6666666666667\n",
      "Eta = 0.0001, momentum = 0.0001, nesterov = True, avg_err = 526.0\n",
      "Eta = 0.0001, momentum = 0.001, nesterov = False, avg_err = 474.0\n",
      "Eta = 0.0001, momentum = 0.001, nesterov = True, avg_err = 508.6666666666667\n",
      "Eta = 0.0001, momentum = 0.01, nesterov = False, avg_err = 491.3333333333333\n",
      "Eta = 0.0001, momentum = 0.01, nesterov = True, avg_err = 491.3333333333333\n",
      "Eta = 0.0001, momentum = 0.1, nesterov = False, avg_err = 514.3333333333334\n",
      "Eta = 0.0001, momentum = 0.1, nesterov = True, avg_err = 508.6666666666667\n",
      "Eta = 0.001, momentum = 0.0001, nesterov = False, avg_err = 491.3333333333333\n",
      "Eta = 0.001, momentum = 0.0001, nesterov = True, avg_err = 491.3333333333333\n",
      "Eta = 0.001, momentum = 0.001, nesterov = False, avg_err = 474.0\n",
      "Eta = 0.001, momentum = 0.001, nesterov = True, avg_err = 474.0\n",
      "Eta = 0.001, momentum = 0.01, nesterov = False, avg_err = 474.0\n",
      "Eta = 0.001, momentum = 0.01, nesterov = True, avg_err = 474.0\n",
      "Eta = 0.001, momentum = 0.1, nesterov = False, avg_err = 491.3333333333333\n",
      "Eta = 0.001, momentum = 0.1, nesterov = True, avg_err = 491.3333333333333\n",
      "Eta = 0.01, momentum = 0.0001, nesterov = False, avg_err = 474.0\n",
      "Eta = 0.01, momentum = 0.0001, nesterov = True, avg_err = 474.0\n",
      "Eta = 0.01, momentum = 0.001, nesterov = False, avg_err = 403.3333333333333\n",
      "Eta = 0.01, momentum = 0.001, nesterov = True, avg_err = 474.0\n",
      "Eta = 0.01, momentum = 0.01, nesterov = False, avg_err = 474.0\n",
      "Eta = 0.01, momentum = 0.01, nesterov = True, avg_err = 474.0\n",
      "Eta = 0.01, momentum = 0.1, nesterov = False, avg_err = 474.0\n",
      "Eta = 0.01, momentum = 0.1, nesterov = True, avg_err = 474.0\n",
      "Eta = 0.1, momentum = 0.0001, nesterov = False, avg_err = 222.66666666666666\n",
      "Eta = 0.1, momentum = 0.0001, nesterov = True, avg_err = 215.0\n",
      "Eta = 0.1, momentum = 0.001, nesterov = False, avg_err = 202.0\n",
      "Eta = 0.1, momentum = 0.001, nesterov = True, avg_err = 209.66666666666666\n",
      "Eta = 0.1, momentum = 0.01, nesterov = False, avg_err = 209.66666666666666\n",
      "Eta = 0.1, momentum = 0.01, nesterov = True, avg_err = 216.33333333333334\n",
      "Eta = 0.1, momentum = 0.1, nesterov = False, avg_err = 211.0\n",
      "Eta = 0.1, momentum = 0.1, nesterov = True, avg_err = 218.33333333333334\n"
     ]
    }
   ],
   "source": [
    "mini_batch_size = 100\n",
    "epochs = 100\n",
    "etas = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "momentum = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "N = 3\n",
    "for eta in etas:\n",
    "    for m in momentum:\n",
    "        for nest in [False, True]:\n",
    "            tot_eta = 0\n",
    "            for i in range(0, N):\n",
    "                model = Net2().to(device)\n",
    "                criterion = nn.CrossEntropyLoss().to(device)\n",
    "                train_model(model, train_input, train_target, mini_batch_size, criterion, epochs, eta, \"SGD\", m, nest)\n",
    "                err = compute_nb_errors(model, test_input, test_target, mini_batch_size)\n",
    "                tot_eta+=err\n",
    "            print(\"Eta = {}, momentum = {}, nesterov = {}, avg_err = {}\".format(eta, m, nest, tot_eta/N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net2().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "mini_batch_size = 100\n",
    "epochs = 250\n",
    "eta = 0.01\n",
    "train_model(model, train_input, train_target, mini_batch_size, criterion, epochs, eta, \"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors(model, train_input, train_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors(model, test_input, test_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize over eta (simple optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta = 1e-05, avg_err = 484.4\n",
      "Eta = 0.0001, avg_err = 202.2\n",
      "Eta = 0.001, avg_err = 210.5\n",
      "Eta = 0.01, avg_err = 200.7\n",
      "Eta = 0.1, avg_err = 474.0\n"
     ]
    }
   ],
   "source": [
    "mini_batch_size = 100\n",
    "epochs = 100\n",
    "etas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "N = 10\n",
    "for eta in etas:\n",
    "    tot_eta = 0\n",
    "    for i in range(0, N):\n",
    "        model = Net2().to(device)\n",
    "        criterion = nn.CrossEntropyLoss().to(device)\n",
    "        train_model(model, train_input, train_target, mini_batch_size, criterion, epochs, eta, \"Adam\")\n",
    "        err = compute_nb_errors(model, test_input, test_target, mini_batch_size)\n",
    "        tot_eta+=err\n",
    "    print(\"Eta = {}, avg_err = {}\".format(eta, tot_eta/N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network 2: siamese network with weight sharing and auxiliary losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self, chan1 = 16, chan2 = 32, chan3 = 64, nb_hidden1 = 100, nb_hidden2 = 50, nb_hidden3 = 50, nb_hidden4 = 10):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential()\n",
    "        self.features.add_module(\"conv_1\", nn.Conv2d(1, chan1, kernel_size=3))\n",
    "        self.features.add_module(\"relu_1\", nn.ReLU())\n",
    "        self.features.add_module(\"maxpool_1\", nn.MaxPool2d(kernel_size=2))\n",
    "        self.features.add_module(\"conv_2\", nn.Conv2d(chan1, chan2, kernel_size=2))\n",
    "        self.features.add_module(\"relu_2\", nn.ReLU())\n",
    "        self.features.add_module(\"maxpool2\", nn.MaxPool2d(kernel_size=2, dilation=1))\n",
    "        self.features.add_module(\"conv_3\", nn.Conv2d(chan2, chan3, kernel_size=2))\n",
    "        self.features.add_module(\"relu_3\", nn.ReLU())\n",
    "        \n",
    "        \n",
    "        class_size = self.features(torch.empty((1, 1, 14, 14))).shape\n",
    "        self.linear_size = reduce(multiplicator, list(class_size[1:]))\n",
    "        \n",
    "        self.classifier1 = nn.Sequential()\n",
    "        self.classifier1.add_module(\"linear_1\", nn.Linear(self.linear_size, nb_hidden1))\n",
    "        self.classifier1.add_module(\"relu_1\", nn.ReLU())\n",
    "        self.classifier1.add_module(\"linear_2\", nn.Linear(nb_hidden1, nb_hidden2))\n",
    "        self.classifier1.add_module(\"relu_2\", nn.ReLU())\n",
    "        self.classifier1.add_module(\"linear_3\", nn.Linear(nb_hidden2, 10))\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.classifier2 = nn.Sequential()\n",
    "        self.classifier2.add_module(\"linear_1\", nn.Linear(10, nb_hidden3))\n",
    "        self.classifier2.add_module(\"relu_1\", nn.ReLU())\n",
    "        self.classifier2.add_module(\"linear_2\", nn.Linear(nb_hidden3, nb_hidden4))\n",
    "        self.classifier2.add_module(\"relu_2\", nn.ReLU())\n",
    "        self.classifier2.add_module(\"linear_3\", nn.Linear(nb_hidden4, 2))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out_aux = []\n",
    "        for i in range(0, 2):\n",
    "            x_i = x[:,i,:,:].view((x.shape[0], 1) + tuple(x.shape[2:]))\n",
    "            x_i = self.features(x_i)\n",
    "            out_aux.append(self.classifier1(x_i.view(-1, self.linear_size)))\n",
    "        diff = out_aux[1] - out_aux[0]\n",
    "        out = self.classifier2(diff)\n",
    "        return out, out_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1)) output shape:\t torch.Size([1, 16, 12, 12])\n",
      "ReLU() output shape:\t torch.Size([1, 16, 12, 12])\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) output shape:\t torch.Size([1, 16, 6, 6])\n",
      "Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1)) output shape:\t torch.Size([1, 32, 5, 5])\n",
      "ReLU() output shape:\t torch.Size([1, 32, 5, 5])\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) output shape:\t torch.Size([1, 32, 2, 2])\n",
      "Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1)) output shape:\t torch.Size([1, 64, 1, 1])\n",
      "ReLU() output shape:\t torch.Size([1, 64, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Check on the sizes\n",
    "X = torch.empty((1, 1, 14, 14)).normal_()\n",
    "net = SiameseNet()\n",
    "for layer in net.features:\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        X = X.view(-1, reduce(multiplicator, list(X.shape[1:])))\n",
    "    X = layer(X)\n",
    "    print(layer, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_siamese(model, train_input, train_target, train_classes, mini_batch_size, criterion, epochs, eta, optim=\"SGD\", momentum = 0, nesterov = False):\n",
    "    \n",
    "    if (optim == \"SGD\"):\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr = eta, momentum = momentum, nesterov = nesterov)\n",
    "    if (optim == \"Adam\"):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = eta)\n",
    "        \n",
    "    for e in range(0, epochs):\n",
    "        for input_data, target_data, class_data in zip(train_input.split(mini_batch_size), train_target.split(mini_batch_size), train_classes.split(mini_batch_size)):\n",
    "            output, out_aux = model(input_data)\n",
    "            loss_out = criterion(output, target_data)\n",
    "            loss_aux0 = criterion(out_aux[0], class_data[:,0])\n",
    "            loss_aux1 = criterion(out_aux[1], class_data[:,1])\n",
    "            loss = loss_out + loss_aux0 + loss_aux1\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_siamese(model, data_input, data_target, mini_batch_size):\n",
    "    tot_err = 0\n",
    "    for input_data, target_data in zip(data_input.split(mini_batch_size), data_target.split(mini_batch_size)):\n",
    "        res, _= model(input_data)\n",
    "        for i, r in enumerate(res):\n",
    "            pred = r.max(0)[1].item()\n",
    "            if(target_data[i])!=pred:\n",
    "                tot_err+=1\n",
    "    return tot_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "model = SiameseNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "mini_batch_size = 100\n",
    "epochs = 250\n",
    "eta = 0.01\n",
    "train_siamese(model, train_input, train_target, train_classes, mini_batch_size, criterion, epochs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors_siamese(model, train_input, train_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors_siamese(model, test_input, test_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize over eta (simple optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta = 1e-05, avg_err = 505.2\n",
      "Eta = 0.0001, avg_err = 505.9\n",
      "Eta = 0.001, avg_err = 484.4\n",
      "Eta = 0.01, avg_err = 255.6\n",
      "Eta = 0.1, avg_err = 89.6\n"
     ]
    }
   ],
   "source": [
    "mini_batch_size = 100\n",
    "epochs = 100\n",
    "etas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "N = 10\n",
    "for eta in etas:\n",
    "    tot_eta = 0\n",
    "    for i in range(0, N):\n",
    "        del model\n",
    "        model = SiameseNet().to(device)\n",
    "        criterion = nn.CrossEntropyLoss().to(device)\n",
    "        train_siamese(model, train_input, train_target, train_classes, mini_batch_size, criterion, epochs, eta)\n",
    "        err = compute_nb_errors_siamese(model, test_input, test_target, mini_batch_size)\n",
    "        tot_eta+=err\n",
    "    print(\"Eta = {}, avg_err = {}\".format(eta, tot_eta/N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize over eta, momentum, nesterov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta = 0.0001, momentum = 0.0001, nesterov = False, avg_err = 508.6666666666667\n",
      "Eta = 0.0001, momentum = 0.0001, nesterov = True, avg_err = 491.3333333333333\n",
      "Eta = 0.0001, momentum = 0.001, nesterov = False, avg_err = 508.6666666666667\n",
      "Eta = 0.0001, momentum = 0.001, nesterov = True, avg_err = 491.3333333333333\n",
      "Eta = 0.0001, momentum = 0.01, nesterov = False, avg_err = 491.3333333333333\n",
      "Eta = 0.0001, momentum = 0.01, nesterov = True, avg_err = 491.3333333333333\n",
      "Eta = 0.0001, momentum = 0.1, nesterov = False, avg_err = 474.0\n",
      "Eta = 0.0001, momentum = 0.1, nesterov = True, avg_err = 526.0\n",
      "Eta = 0.001, momentum = 0.0001, nesterov = False, avg_err = 474.0\n",
      "Eta = 0.001, momentum = 0.0001, nesterov = True, avg_err = 508.6666666666667\n",
      "Eta = 0.001, momentum = 0.001, nesterov = False, avg_err = 491.3333333333333\n",
      "Eta = 0.001, momentum = 0.001, nesterov = True, avg_err = 474.0\n",
      "Eta = 0.001, momentum = 0.01, nesterov = False, avg_err = 474.0\n",
      "Eta = 0.001, momentum = 0.01, nesterov = True, avg_err = 474.0\n",
      "Eta = 0.001, momentum = 0.1, nesterov = False, avg_err = 491.3333333333333\n",
      "Eta = 0.001, momentum = 0.1, nesterov = True, avg_err = 474.0\n",
      "Eta = 0.01, momentum = 0.0001, nesterov = False, avg_err = 242.66666666666666\n",
      "Eta = 0.01, momentum = 0.0001, nesterov = True, avg_err = 250.0\n",
      "Eta = 0.01, momentum = 0.001, nesterov = False, avg_err = 302.0\n",
      "Eta = 0.01, momentum = 0.001, nesterov = True, avg_err = 280.3333333333333\n",
      "Eta = 0.01, momentum = 0.01, nesterov = False, avg_err = 278.0\n",
      "Eta = 0.01, momentum = 0.01, nesterov = True, avg_err = 249.0\n",
      "Eta = 0.01, momentum = 0.1, nesterov = False, avg_err = 264.0\n",
      "Eta = 0.01, momentum = 0.1, nesterov = True, avg_err = 232.66666666666666\n",
      "Eta = 0.1, momentum = 0.0001, nesterov = False, avg_err = 89.0\n",
      "Eta = 0.1, momentum = 0.0001, nesterov = True, avg_err = 83.66666666666667\n",
      "Eta = 0.1, momentum = 0.001, nesterov = False, avg_err = 87.66666666666667\n",
      "Eta = 0.1, momentum = 0.001, nesterov = True, avg_err = 155.0\n",
      "Eta = 0.1, momentum = 0.01, nesterov = False, avg_err = 81.66666666666667\n",
      "Eta = 0.1, momentum = 0.01, nesterov = True, avg_err = 100.66666666666667\n",
      "Eta = 0.1, momentum = 0.1, nesterov = False, avg_err = 85.66666666666667\n",
      "Eta = 0.1, momentum = 0.1, nesterov = True, avg_err = 88.33333333333333\n"
     ]
    }
   ],
   "source": [
    "mini_batch_size = 100\n",
    "epochs = 100\n",
    "etas = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "momentum = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "N = 3\n",
    "for eta in etas:\n",
    "    for m in momentum:\n",
    "        for nest in [False, True]:\n",
    "            tot_eta = 0\n",
    "            for i in range(0, N):\n",
    "                del model\n",
    "                model = SiameseNet().to(device)\n",
    "                criterion = nn.CrossEntropyLoss().to(device)\n",
    "                train_siamese(model, train_input, train_target, train_classes, mini_batch_size, criterion, epochs, eta, \"SGD\", m, nest)\n",
    "                err = compute_nb_errors_siamese(model, test_input, test_target, mini_batch_size)\n",
    "                tot_eta+=err\n",
    "            print(\"Eta = {}, momentum = {}, nesterov = {}, avg_err = {}\".format(eta, m, nest, tot_eta/N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "model = SiameseNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "mini_batch_size = 100\n",
    "epochs = 250\n",
    "eta = 0.01\n",
    "train_siamese(model, train_input, train_target, train_classes, mini_batch_size, criterion, epochs, eta, \"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors_siamese(model, train_input, train_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors_siamese(model, test_input, test_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize over eta (simple optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta = 1e-05, avg_err = 490.5\n",
      "Eta = 0.0001, avg_err = 215.1\n",
      "Eta = 0.001, avg_err = 119.9\n",
      "Eta = 0.01, avg_err = 73.0\n",
      "Eta = 0.1, avg_err = 459.0\n"
     ]
    }
   ],
   "source": [
    "mini_batch_size = 100\n",
    "epochs = 100\n",
    "etas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "N = 10\n",
    "for eta in etas:\n",
    "    tot_eta = 0\n",
    "    for i in range(0, N):\n",
    "        del model\n",
    "        model = SiameseNet().to(device)\n",
    "        criterion = nn.CrossEntropyLoss().to(device)\n",
    "        train_siamese(model, train_input, train_target, train_classes, mini_batch_size, criterion, epochs, eta, \"Adam\")\n",
    "        err = compute_nb_errors_siamese(model, test_input, test_target, mini_batch_size)\n",
    "        tot_eta+=err\n",
    "    print(\"Eta = {}, avg_err = {}\".format(eta, tot_eta/N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[10, 1, 14, 14]' is invalid for input of size 196000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-fc72384b7eba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_input_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_classes_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_one_hot_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_classes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_input_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_classes_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_one_hot_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_classes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[10, 1, 14, 14]' is invalid for input of size 196000"
     ]
    }
   ],
   "source": [
    "train_input_left = train_input[:,0,:,:].view(N,1,14,14)\n",
    "train_classes_left = plg.convert_to_one_hot_labels(train_input, train_classes[:,0]).type(torch.LongTensor)\n",
    "test_input_left = test_input[:,0,:,:].view(N,1,14,14)\n",
    "test_classes_left = plg.convert_to_one_hot_labels(test_input, test_classes[:,0]).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size):\n",
    "    \n",
    "    if (torch.cuda.is_available()):\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "    model.to(device)\n",
    "    train_input = train_input.to(device)\n",
    "    train_target = train_target.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    eta = 1e-1\n",
    "    epochs = 250\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = eta)\n",
    "    \n",
    "    for e in range(0, epochs):\n",
    "        for input_data, target_data in zip(train_input.split(mini_batch_size), train_target.split(mini_batch_size)):\n",
    "            output = model(input_data)\n",
    "            #print(output)\n",
    "            loss = criterion(output, target_data)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, input, target, mini_batch_size):\n",
    "    nb_errors = 0\n",
    "\n",
    "    for b in range(0, input.size(0), mini_batch_size):\n",
    "        output = model(input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = output.max(1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if target[b + k] != predicted_classes[k]:\n",
    "                nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "model.to(device)\n",
    "train_model(model, train_input_left, train_classes[:,0], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of backend CPU but got backend CUDA for argument #2 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-eb007706b898>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcompute_nb_errors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_input_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_classes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-5d63722d00c2>\u001b[0m in \u001b[0;36mcompute_nb_errors\u001b[1;34m(model, input, target, mini_batch_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-3729cbeb8948>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 320\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of backend CPU but got backend CUDA for argument #2 'weight'"
     ]
    }
   ],
   "source": [
    "compute_nb_errors(model, train_input_left, test_classes[:,0], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
